#!/usr/bin/env ruby

require 'faraday_middleware'
require 'typhoeus/adapters/faraday'
require 'multi_json'
require 'ruby-progressbar'
require 'terminal-table'
require 'logger'
require 'csv'
require 'byebug'
require 'dotenv'
Dotenv.load

logger       = Logger.new(File.expand_path('../../log/development.log', __FILE__))
requests_log = Logger.new(File.expand_path('../../log/requests.log', __FILE__))
report_name = ENV.fetch('REPORT_NAME')

connection = Faraday.new(url: ENV.fetch('API_ENDPOINT')) do |c|
  c.response(:logger, requests_log)
  c.adapter(:typhoeus)
end

connection.headers['Content-Type']  = 'application/json'
connection.headers['Accept']        = 'application/json'
connection.headers['Authorization'] = "Bearer %s" % [ENV.fetch('API_KEY')]

body = ENV.fetch('JSON_BLOB')
report_url = "/v2/products/%s/reports/people_search" % [ENV.fetch('PRODUCT_ID')]
request = connection.post(report_url,body)

case(request.status)
when(202)
  status_url = request.headers.fetch('location')
  query_params = {
    :limit   => ENV.fetch('RESULTS_LIMIT', 100)
  }
  status_request = connection.get(status_url, query_params)

  case(status_request.status)
  when 201
  when 200, 201
    logger.info("Running") { "Polling the status from %s" % [status_url] }
    progress_bar = ProgressBar.create(title: 'Results status', starting_at: 0, total: 1.0)

    next_request = loop do
      poll_status_request  = connection.get(status_url, query_params)

      poll_status_response = MultiJson.load(poll_status_request.body)
      break poll_status_request if poll_status_response['data']['completed'] == true || (poll_status_response.has_key?('error') && !poll_status_response['error'].nil?)
      progress_bar.progress = poll_status_response['data']['progress']
      logger.info("Running") { "Progress: %s" % [poll_status_response['data']['progress']] }
    end
    progress_bar.finish

    case(next_request.status)
    when 201
      results_url = next_request.headers.fetch('location')

      logger.info("Running") { "Retrieving results from %s" % [results_url] }

      results_request = connection.get(results_url, query_params)

      case(results_request.status)
      when 200
        rows = []

        results_response = MultiJson.load(results_request.body)

        rows.concat(results_response.fetch('data', []))

        links     = results_response.fetch('links', [])
        next_link = links.select { |r| r['rel'] == 'next' }.first

        if next_link
          loop do
            logger.info("Running") { "Retrieving paginated set %s" % [next_link['href']] }

            results_request  = connection.get(next_link['href'])
            results_response = MultiJson.load(results_request.body)

            links     = results_response.fetch('links', [])
            next_link = links.select { |r| r['rel'] == 'next' }.first

            rows.concat(results_response.fetch('rows', []))
            break if next_link.nil?
          end
        end

        header_display = ->(row) do
          display_return_option = row.fetch('display_return_option', nil)

          val = ''
          val << row.fetch('display_name', nil)

          if display_return_option
            val << " (%s)" % [display_return_option]
          end

          val
        end

        columns  = results_response.fetch('columns', [])
        headings = columns.map { |r| header_display.call(r) }
         #Output to the console
        table = Terminal::Table.new(title: 'Query Results', headings: headings, rows: rows)
        puts table.to_s

        # Write to a final JSON file
        final_response_body = {
          :total    => results_response.fetch('total', 0),  
          :metadata => results_response.fetch('metadata', {}),
          :columns  => columns,
          :rows     => rows
        }

        logger.info("Running") { "Writing to cache/#{report_name}.json cache" }
        File.open(File.expand_path("../../cache/#{report_name}.json", __FILE__), 'w+') do |f|
          f.write(MultiJson.dump(final_response_body))
        end
        header=["ID","Identity"]
        logger.info("Running") { "Writing to cache/#{report_name}.csv cache" }
        CSV.open(File.expand_path("../../cache/#{report_name}.csv", __FILE__), 'w+') do |csv|
          header.push(headings) if columns.length != 0
          csv<<header
          rows.each { |r| csv<<[r["__kmid"],r["identity"],r["columns"]] }
        end
      else
        logger.error("Running") { "[Results Request] There was an error (%s)\n%s" % [results_request.status, results_request.inspect] }
      end
    else
      logger.error("Running") { "[Status Ping Request] There was an error (%s)\n%s" % [next_request.status, next_request.inspect] }
    end
  else
    logger.error("Running") { "[Initial Status Request] There was an error (%s)\n%s" % [status_request.status, status_request.inspect] }
  end
else
  logger.error("Running") { "[Initial Request] There was an error (%s)\n%s" % [request.status, request.inspect] }
end

